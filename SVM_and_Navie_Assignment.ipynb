{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 1\n",
        "\n",
        "**Definition**\n",
        "\n",
        "Information Gain is the reduction in entropy (uncertainty or impurity) achieved by splitting the dataset based on a feature.\n",
        "\n",
        "\n",
        "**How Information Gain Works in Decision Trees**\n",
        "\n",
        "1. Calculate the entropy of the entire dataset (before splitting).\n",
        "\n",
        "2. For each feature:\n",
        "\n",
        "* Split the data based on that feature’s possible values.\n",
        "\n",
        "* Compute the entropy of each subset.\n",
        "\n",
        "* Calculate the weighted average entropy after the split.\n",
        "\n",
        "* Find the Information Gain.\n",
        "\n",
        "3. Choose the feature with the highest Information Gain to make the split — this feature gives the most “information” about the target."
      ],
      "metadata": {
        "id": "2I4HbhHmsjty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 2\n",
        "\n",
        "Here are the key differences between Gini Impurity and Entropy;\n",
        "\n",
        "1. Gini Impurity:\n",
        "\n",
        "* Used squares (probability of miss classification)\n",
        "\n",
        "* Computation is faster.\n",
        "\n",
        "* Often gives similar results to Entropy.\n",
        "\n",
        "2. Entropy:\n",
        "\n",
        "* Uses logarithms (information theory)\n",
        "\n",
        "* Computation is slightly slower.\n",
        "\n",
        "* Can favor splits with more distinct values."
      ],
      "metadata": {
        "id": "4vLDR8jjtnHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 3\n",
        "\n",
        "**What It Means**\n",
        "\n",
        "When building a Decision Tree, the algorithm keeps splitting the data into smaller and smaller subsets to make predictions more accurate.\n",
        "However, if it splits too much, the tree starts to memorize noise instead of learning general patterns — this is called overfitting.\n",
        "\n",
        "* Pre-Pruning stops the tree from growing once certain conditions are met, to keep it simpler and more generalizable."
      ],
      "metadata": {
        "id": "QJwzQsgfu6Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Answer 4\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load a sample dataset (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data          # Feature matrix\n",
        "y = data.target        # Target labels\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train (fit) the model\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFSV8BV0v2lK",
        "outputId": "d98146dd-804b-4f25-d0dc-f5c0d9bbdbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 5\n",
        "\n",
        "SVM tries to find the **best boundary** (**hyperplane**) that separates data points of different classes with the **maximum margin**(the largest possible gap between the classes).\n",
        "\n",
        "So, it’s all about finding a line (in 2D) or a plane/hyperplane (in higher dimensions) that clearly divides the classes."
      ],
      "metadata": {
        "id": "DitoQC3FwMGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 6\n",
        "\n",
        "* **The Problem**\n",
        "\n",
        "In a Linear SVM, the model finds a straight line (or hyperplane) to separate classes.\n",
        "But what if the data can’t be separated by a straight line?\n",
        "\n",
        "Example:\n",
        "Imagine data shaped like two concentric circles — you can’t separate them with a line in 2D space.\n",
        "\n",
        "\n",
        "* **The Idea Behind the Kernel Trick**\n",
        "\n",
        "Instead of trying to separate the data in the original (low-dimensional) space,\n",
        "SVM maps **the data to a higher-dimensional space**, where it becomes separable by a linear boundary.\n",
        "\n",
        "This mapping is done implicitly using a Kernel Function, without actually computing the new coordinates — that’s the **“trick.”**\n",
        "\n",
        "\n",
        "* **What Is a Kernel Function?**\n",
        "\n",
        "A kernel function computes the dot product (similarity) between two data points in a higher-dimensional space, **without explicitly transforming** the data."
      ],
      "metadata": {
        "id": "6YlK9whawpbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Answer 7\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data        # Features\n",
        "y = data.target      # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Create two SVM classifiers\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train both classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print comparison results\n",
        "print(\"SVM Classifier Comparison on Wine Dataset\")\n",
        "print(\"------------------------------------------\")\n",
        "print(f\"Linear Kernel Accuracy: {acc_linear:.4f}\")\n",
        "print(f\"RBF Kernel Accuracy:    {acc_rbf:.4f}\")\n",
        "\n",
        "# (Optional) Print which one performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"\\n✅ Linear Kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"\\n✅ RBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"\\n⚖️  Both kernels performed equally well.\")"
      ],
      "metadata": {
        "id": "ke8e96VJxbmt",
        "outputId": "fc40181b-9629-43f3-b1b9-1c64d586d94b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Comparison on Wine Dataset\n",
            "------------------------------------------\n",
            "Linear Kernel Accuracy: 0.9444\n",
            "RBF Kernel Accuracy:    0.6944\n",
            "\n",
            "✅ Linear Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 8\n",
        "\n",
        "* **Definition**\n",
        "\n",
        "\n",
        "The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem, used for classification tasks.\n",
        "It predicts the class of a given data point based on the probability of it belonging to each class.\n",
        "\n",
        "* **Example**\n",
        "\n",
        "Suppose we want to predict whether an email is **Spam or Not Spam**, based on words like:\n",
        "\n",
        "“Free,” “Offer,” “Buy,” etc.\n",
        "\n",
        "\n",
        "Even though words may appear together, Naïve Bayes assumes each word contributes **independently** to the probability of the email being spam.\n",
        "\n",
        "* **Why Important**\n",
        "\n",
        "1. Simple and fast to train\n",
        "\n",
        "\n",
        "2.  Works well with high-dimensional data (like text)\n",
        "\n",
        "\n",
        "3.  Requires small training data\n",
        "\n",
        "4. Performs well even with the “naïve” independence assumption"
      ],
      "metadata": {
        "id": "pQ1P7TkcyANg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Answer 9\n",
        "\n",
        "Here are the key differences pointwise between three;\n",
        "\n",
        "\n",
        "1. **Gaussian Naïve Bayes**\n",
        "\n",
        "* Assumes features follow a normal (bell-shaped) distribution.\n",
        "\n",
        "* Suitable for continuous numerical data.\n",
        "\n",
        "* Each feature is modeled using a mean (μ) and standard deviation (σ) per class.\n",
        "\n",
        "\n",
        "2. **Multinomial Naïve Bayes**\n",
        "\n",
        "* Assumes features represent counts or frequencies.\n",
        "\n",
        "* Suitable for discrete, non-negative integer features, such as word counts in text.\n",
        "\n",
        "* Often used for document classification.\n",
        "\n",
        "\n",
        "3. **Bernoulli Naïve Bayes**\n",
        "\n",
        "* Assumes binary features (values are 0 or 1).\n",
        "\n",
        "* Suitable when we only care about whether a feature is present or absent, not how many times it occurs.\n",
        "\n",
        "* Models each feature with a Bernoulli distribution."
      ],
      "metadata": {
        "id": "tFtaQzISzEiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Answer 10\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data        # Features\n",
        "y = data.target      # Labels (0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Display results\n",
        "print(\"Gaussian Naïve Bayes on Breast Cancer Dataset\")\n",
        "print(\"---------------------------------------------\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
        "\n",
        "# Optional: Detailed performance report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7CDm5zE0WF6",
        "outputId": "af619076-6803-465d-b68e-080eea31fc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naïve Bayes on Breast Cancer Dataset\n",
            "---------------------------------------------\n",
            "Accuracy: 0.9386\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        42\n",
            "      benign       0.95      0.96      0.95        72\n",
            "\n",
            "    accuracy                           0.94       114\n",
            "   macro avg       0.94      0.93      0.93       114\n",
            "weighted avg       0.94      0.94      0.94       114\n",
            "\n"
          ]
        }
      ]
    }
  ]
}